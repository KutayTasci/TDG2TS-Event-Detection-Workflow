{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8833ac8",
   "metadata": {},
   "source": [
    "# Transforming Temporal-Dynamic Graphs Into Time-Series Data for Solving Event Detection Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be157791",
   "metadata": {},
   "source": [
    "Event detection problems on temporal-dynamic graphs aim to detect important events by detection abnormal changes on the\n",
    "network. Because of the excessive use of social media, many real world problems can be modelled as temporal-dynamic graph\n",
    "data. With the recent progress in graph representation learning, new anomaly detection on static graphs are studied. In this\n",
    "work, we present a workflow for event detection on temporal-dynamic graphs with using graph representation learning.\n",
    "Our workflow uses generated embeddings of the temporal-dynamic graph to transform the problem into a unsupervised\n",
    "time-series anomaly detection problem. Since this is a widely studied research area, transforming temporal-dynamic graph\n",
    "data into multivariate time series data, provides many possible solutions for the event detection problems. We have evaluated\n",
    "our proposed workflow on four different real-world datasets and compared our results. Our workflow shows competitive per-\n",
    "formance, when compared to previous studies. This study gives a proof of concept for using graph embeddings as time-series\n",
    "data in anomaly detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf48490",
   "metadata": {},
   "source": [
    "# Proposed Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f340e7",
   "metadata": {},
   "source": [
    "In the figure bellow you can see the proposed model workflow. Input is a temporal-dynamic graph G which consists of static\n",
    "snapshots of the graph taken in different time steps.Then model generates n-dimensional vector embeddings from given\n",
    "input graph, with using graph representation learning. After this step model pass these embeddings to an unsupervised\n",
    "anomaly detector. Output of proposed workflow is the anomaly scores corresponding to each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd54af7",
   "metadata": {},
   "source": [
    "<img src=\"Proposed_Workflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ae314",
   "metadata": {},
   "source": [
    "In following experiments, we are going to use our proposed workflow. After pre-processing our data, first step is\n",
    "to generate graph embeddings. For this task we used tdGraphEmbed model. Model generates 40 random-walks\n",
    "for each node in the graph and length of each walk is 16 in our experiments. The model is trained 50 iterations,\n",
    "with generated random-walk document. In the second step we are going to use time-series anomaly detectors we\n",
    "mentioned above. For these algorithms we used Merlion machine learning library for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7db4c9",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1f3dc",
   "metadata": {},
   "source": [
    "Before importing libraries, make sure to install requirements in Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eeee8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TR\\anaconda3\\envs\\tdGraphEmbed\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from merlion.utils import TimeSeries\n",
    "from evaluation_util import *\n",
    "\n",
    "from tdGraphEmbed.tdGraphEmbed.temporal_graph import TemporalGraph\n",
    "from tdGraphEmbed.tdGraphEmbed.model import TdGraphEmbed\n",
    "from datasetConverter import dataset_convert\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1e26e",
   "metadata": {},
   "source": [
    "# Generating Temporal-Dynamic Graph Embeddings\n",
    "### !Important Note: Training of this section takes aproximately 15-24 hours. But, pretrained models and embeddings are available. Optionally you can skip this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4dd33",
   "metadata": {},
   "source": [
    "In this step we are going to read the data from files and generate a model for training process. For getting the datasets you, we will use the data_conver() function. Available datasets are:\n",
    "\n",
    "Tw-WorldCup - The Twitter WorldCup datasets. In experiments you can use granularity as hours.\n",
    "\n",
    "Tw-Terror-Security - The Twitter Terror Security. In experiments you can use granularity as days.\n",
    "\n",
    "gameofthrones - The Reddit Game of Thrones. You can read directly, since this dataset is provided as picke file.\n",
    "\n",
    "formula - The Reddit Formula 1. You can read directly, since this dataset is provided as picke file.\n",
    "\n",
    "!!!You can find the datasets with size more than 100MB in https://drive.google.com/drive/folders/1D8P9LBHXERWN_r-hiTWNU4HDe3VmVHbx?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7091198",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = dataset_convert(dataset=\"Tw-WorldCup\",granularity=\"hours\")\n",
    "model = TdGraphEmbed(dataset_name=\"Tw-WorldCup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = model.get_documents_from_graph(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_doc2vec(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vectors = model.get_embeddings()\n",
    "np.save(\"tdGraphEmbed/saved_embeddings/Tw-WorldCup.npy\", graph_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8011f92",
   "metadata": {},
   "source": [
    "In the code above you can read the dataset and train the graph representation learning model. Then save the model and embeddings into files. Training for this process takes a long time around 15-24 hours to complete. Because of this we will use the model and files we have saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b37134",
   "metadata": {},
   "source": [
    "# Unsupervised Time Series Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100326a6",
   "metadata": {},
   "source": [
    "Bellow, we will read the saved model and dataset labels and prepares the data in the format of Merlion library. Saved model file will provide us with time-stamps and n-dimensional embeddings and we will use the labels to evaluate our model. All training process is fully unsupervised and we only use labels for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f624eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n"
     ]
    }
   ],
   "source": [
    "model_path = \"tdGraphEmbed/trained_models/Tw-WorldCup.model\"\n",
    "labels_path = \"Datasets/Twitter_WorldCup/Twitter_WorldCup_2014_labels.txt\"\n",
    "\n",
    "#model_path = \"tdGraphEmbed/trained_models/Tw-Terror-Security.model\"\n",
    "#labels_path = \"Datasets/Twitter_Security/Twitter_May_Aug_2014_TerrorSecurity_labels.txt\"\n",
    "\n",
    "#model_path = \"tdGraphEmbed/trained_models/GoT-2017.model\"\n",
    "#labels_path = \"Datasets/gameofthrones/gameofthrones_2017_labels.txt\"\n",
    "\n",
    "#model_path = \"tdGraphEmbed/trained_models/Formula-2019.model\"\n",
    "#labels_path = \"Datasets/formula/formula_2019_labels.txt\"\n",
    "\n",
    "\n",
    "\n",
    "model = Doc2Vec.load(model_path)\n",
    "doc_vecs = model.docvecs.doctag_syn0\n",
    "doc_vecs = doc_vecs[np.argsort([model.docvecs.index_to_doctag(i) for i in range(0, doc_vecs.shape[0])])]\n",
    "\n",
    "time_stamps = list(model.docvecs.doctags.keys())\n",
    "time_series_custom = pd.DataFrame(doc_vecs, index=time_stamps)\n",
    "\n",
    "ls = readFiles(labels_path, granularity=\"hours\")\n",
    "df_metadata = pd.DataFrame(columns = ['trainval', 'anomaly'], index = time_stamps)\n",
    "df_metadata = generate_metadata(df_metadata, time_stamps, ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df110bb",
   "metadata": {},
   "source": [
    "In this step we are going to train our unsupervised time-series anomaly detection model. There are some different settings for different datasets. Bellow the settings are adjusted for The Twitter World-Cup dataset. Overall important parameters are; \"top\" variable is the k variable in Recall@k and Precision@k and defines the amount of anomalies to detect. Available models we recommend are \"VAE\", \"LSTMED\", and \"IsolationForest\". For other datasets please follow the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c10b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |████████████████████████████████████████| 100.0% Complete, Loss 0.0013\n",
      "Anomaly Threshold: \n",
      "anom_score    1.995362\n",
      "Name: 2014-06-14 19:00:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from merlion.models.factory import ModelFactory\n",
    "from merlion.post_process.threshold import AggregateAlarms\n",
    "\n",
    "# @k parameter to set\n",
    "top=30\n",
    "\n",
    "train_data = TimeSeries.from_pd(time_series_custom[:])\n",
    "test_labels = TimeSeries.from_pd(df_metadata[\"anomaly\"][:])\n",
    "\n",
    "#Available models are VAE, LSTMED, and IsolationForest\n",
    "#Because of the data properties Isolation Forest is best in Twitter Security dataset.\n",
    "#It is best to use VAE or LSTMED on other datasets.\n",
    "model = ModelFactory.create(\"VAE\",\n",
    "                            threshold=AggregateAlarms(alm_threshold=0))\n",
    "\n",
    "model.train(train_data)\n",
    "labels = model.get_anomaly_label(train_data)\n",
    "df_temp = labels.to_pd()\n",
    "df_cpy = df_temp.copy()\n",
    "\n",
    "#For datasets other than The Twitter World-Cup ascending should be True. This is due to some\n",
    "#implementation error on Merlion library.\n",
    "df_temp = get_top_anomalies(df_temp,ascending=False , top=top)\n",
    "\n",
    "#If you are using Isolation Forest Model you have to use test_labels_temp = test_labels[1:]\n",
    "#Because ısolation forest does nor return an anomaly score for first time-stamp.\n",
    "test_labels_temp = test_labels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e380aaa",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this part you can evaluate the model with precision and recall. Also you can try out different delay factors. Since model training is trivial. You can get different results in significance interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f9c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 time-stamps are considered as anomalies\n",
      "Precision: 0.6333333333333333\n",
      "Recall: 0.15966386554621848\n",
      "Accuracy: 0.9380234505862647\n"
     ]
    }
   ],
   "source": [
    "prec = get_precision(df_temp, test_labels_temp,delay=0)\n",
    "rec = get_recall(df_temp, test_labels_temp,delay=0,top=top)\n",
    "acc = get_accuracy(df_temp, test_labels_temp)\n",
    "print(\"Top\",top,\"time-stamps are considered as anomalies\")\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e8d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision and Recall with delay 0\n",
      "Precision: 0.6333333333333333\n",
      "Recall: 0.15966386554621848\n",
      "---------\n",
      "Precision and Recall with delay 1\n",
      "Precision: 0.9333333333333333\n",
      "Recall: 0.453781512605042\n",
      "---------\n",
      "Precision and Recall with delay 2\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.5546218487394958\n",
      "---------\n",
      "Precision and Recall with delay 3\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.6470588235294118\n",
      "---------\n",
      "Precision and Recall with delay 4\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.7394957983193278\n",
      "---------\n",
      "Precision and Recall with delay 5\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.7815126050420168\n",
      "---------\n",
      "Precision and Recall with delay 6\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.7899159663865546\n",
      "---------\n",
      "Precision and Recall with delay 7\n",
      "Precision: 1.0\n",
      "Recall: 0.7899159663865546\n",
      "---------\n",
      "Precision and Recall with delay 8\n",
      "Precision: 1.0\n",
      "Recall: 0.7899159663865546\n",
      "---------\n",
      "Precision and Recall with delay 9\n",
      "Precision: 1.0\n",
      "Recall: 0.7899159663865546\n",
      "---------\n",
      "Precision and Recall with delay 10\n",
      "Precision: 1.0\n",
      "Recall: 0.7899159663865546\n",
      "---------\n",
      "Precision and Recall with delay 11\n",
      "Precision: 1.0\n",
      "Recall: 0.7899159663865546\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):  \n",
    "    prec = get_precision(df_temp, test_labels_temp,delay=i)\n",
    "    rec = get_recall(df_temp, test_labels_temp,delay=i,top=top)\n",
    "    print(\"Precision and Recall with delay\",i)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"---------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6bc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
